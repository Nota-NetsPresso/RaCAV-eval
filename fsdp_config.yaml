# log parameters
exp_name: "llama-3-8b"
output_base_dir: "./output"                     # Temporary output directory for model checkpoints
# script parameters
model_id: "meta-llama/Meta-Llama-3-8B-Instruct" # Hugging Face model id
dataset_path: "PLAV"         # path to dataset
max_seq_length: 4096                            # max sequence length for model and packing of the dataset
train_task: "single_token"                      # ['raw', 'single_token']
# training parameters
output_dir: "./output"
report_to: "wandb"                              # report metrics to wandb
learning_rate: 0.0002                           # learning rate 2e-4
lr_scheduler_type: "cosine"                     # learning rate scheduler
num_train_epochs: 10                            # number of training epochs
per_device_train_batch_size: 4                 # batch size per device during training
per_device_eval_batch_size: 4                  # batch size for evaluation
gradient_accumulation_steps: 4                  # number of steps before performing a backward/update pass
optim: adamw_torch                              # use torch adamw optimizer
logging_steps: 1                                # log every 1 steps
save_strategy: steps                            # save checkpoint every 50 steps 
save_steps: 50
save_total_limit: 5
evaluation_strategy: steps                      # evaluate every 50 steps 
eval_steps: 50
max_grad_norm: 0.3                              # max gradient norm
warmup_ratio: 0.1                               # warmup ratio
bf16: true                                      # use bfloat16 precision
tf16: true                                      # use tf32 precision
gradient_checkpointing: true                    # use gradient checkpointing to save memory
metric_for_best_model: "eval_plav_loss"
greater_is_better: false
# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp
fsdp: "full_shard auto_wrap offload"            # remove offload if enough GPU memory
fsdp_config:
  backward_prefetch: "backward_pre"
  forward_prefetch: "false"
  use_orig_params: "false"
num_processes: 4
mixed_precision: 'yes'